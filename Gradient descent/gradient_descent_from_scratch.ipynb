{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gradient Descent from Scratch\n",
                "\n",
                "In this notebook, we will implement linear regression using gradient descent without using `sklearn`.\n",
                "\n",
                "## Steps:\n",
                "1. Initialize parameters\n",
                "2. Compute predictions\n",
                "3. Compute MSE loss\n",
                "4. Update parameters using gradients\n",
                "5. Run for multiple epochs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Set seed for reproducibility\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Generation\n",
                "We will generate synthetic data for simple linear regression: $y = 4 + 3x + noise$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = 2 * np.random.rand(100, 1)\n",
                "y = 4 + 3 * X + np.random.randn(100, 1)\n",
                "\n",
                "# Visualize the data\n",
                "plt.scatter(X, y)\n",
                "plt.xlabel('X')\n",
                "plt.ylabel('y')\n",
                "plt.title('Synthetic Data')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Initialization\n",
                "Initialize separate parameters for w (weight/slope) and b (bias/intercept)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize w and b randomly\n",
                "w = np.random.randn(1)\n",
                "b = np.random.randn(1)\n",
                "\n",
                "print(f\"Initial w: {w}\")\n",
                "print(f\"Initial b: {b}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop\n",
                "We will implement the gradient descent steps inside the loop:\n",
                "- Prediction: $\\hat{y} = Xw + b$\n",
                "- Loss (MSE): $J(w,b) = \\frac{1}{n} \\sum (\\hat{y} - y)^2$\n",
                "- Gradients:\n",
                "    - $\\frac{\\partial J}{\\partial w} = \\frac{2}{n} \\sum (\\hat{y} - y)x$\n",
                "    - $\\frac{\\partial J}{\\partial b} = \\frac{2}{n} \\sum (\\hat{y} - y)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "learning_rate = 0.1\n",
                "n_epochs = 1000\n",
                "\n",
                "losses = []\n",
                "\n",
                "for epoch in range(n_epochs):\n",
                "    # 1. Compute predictions\n",
                "    y_pred = X.dot(w) + b\n",
                "    \n",
                "    # 2. Compute MSE loss\n",
                "    n = len(X)\n",
                "    loss = (1/n) * np.sum((y_pred - y)**2)\n",
                "    losses.append(loss)\n",
                "    \n",
                "    # 3. Compute gradients\n",
                "    dw = (2/n) * np.sum((y_pred - y) * X)\n",
                "    db = (2/n) * np.sum(y_pred - y)\n",
                "    \n",
                "    # 4. Update parameters\n",
                "    w = w - learning_rate * dw\n",
                "    b = b - learning_rate * db\n",
                "    \n",
                "    if epoch % 100 == 0:\n",
                "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
                "\n",
                "print(f\"\\nFinal w: {w}\")\n",
                "print(f\"Final b: {b}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualization\n",
                "Let's look at the learning curve and the final fit."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot Learning Curve\n",
                "plt.plot(losses)\n",
                "plt.xlabel(\"Epochs\")\n",
                "plt.ylabel(\"MSE Loss\")\n",
                "plt.title(\"Gradient Descent Training Loss\")\n",
                "plt.show()\n",
                "\n",
                "# Plot Final Fit\n",
                "plt.scatter(X, y, label='Data')\n",
                "plt.plot(X, X.dot(w) + b, color='red', label='Prediction')\n",
                "plt.xlabel('X')\n",
                "plt.ylabel('y')\n",
                "plt.legend()\n",
                "plt.title(\"Linear Regression Fit\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}